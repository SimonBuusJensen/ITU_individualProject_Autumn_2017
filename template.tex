\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage[final]{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{color}

\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{mygreen}{rgb}{0,0.6,0}

\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{7} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{7}  % for normal

\lstset{ 
otherkeywords={name, shape, dtype, initializer, value, filters, stride, padding},
breaklines=true,
tabsize=1,
language=Python,
basicstyle=\ttm,
commentstyle=\color{mygreen},
keywordstyle=\ttb\color{deepblue},
stringstyle=\color{deepgreen},
showstringspaces=false,
emphstyle=\ttb\color{deepred},
keepspaces=false
}  

\title{Object Detection: How Noise Patterns Affect the Performance of Convolutional Neural Networks}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Simon B. Jensen\\
  Software Development \& Technology\\
  IT University of Copenhagen\\
  \texttt{sije@itu.dk} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\section{Problem Formulation}
\label{sec:problem-formulation}
In the last few years Object Detection challenges such as the Pascal Visual Object Classes Challenge (Pascal VOC)\footnote{http://host.robots.ox.ac.uk/pascal/VOC/} and the ImageNet Large Scale Visual Recognition Competition (ILSVRC)\footnote{http://www.image-net.org/challenges/LSVRC/} have seen tremendous increases in detection performances due to new development in Deep Learning methods. More specifically development in Convolutional Neural Networks (CNNs) - a specific branch of Deep Learning. The performance of the Object Detecting CNNs are now at a point where they have even surpassed human capabilities on these specific challenges. However, one can argue that the images of the challenges are not realistic as the images are ideally cropped, non-blurry and mostly only contain a few specific objects, for which the CNNs can be optimized for. The human visual system is still far superior when the settings are more realistic and non-ideal. What happens for example if we add different noise patterns to the images in the Object Detection Challenges? By adding noise patterns to the images we try to simulate a realistic and non-ideal setting. A non-ideal setting could for instance be a slightly blurred image, an incomplete image or a damaged image etc. Which we as humans typically easily can deviate from and still be able to identify what is in the image. 
For this Individual Project we will investigate how different noise patterns influences the detection performance of state-of-the-art object detection system. 

\section{Image based data sets: Pascal VOC}
The following section introduces and describes the Pascal VOC data set and it's role as a benchmark data set. The section can be skipped if the reader is already familiar with the data set.

There exists only a handful of image based data sets which are openly available to the public, and where the images and corresponding labels are both of high quality and there are a substantial amount of them. Thereby containing the key properties for serving as a valid benchmark data set. The Pascal VOC image data set is such a data set. Other such data sets are e.g. The ImageNet, COCO \footnote{http://cocodataset.org/\#home} and KITTI Vision \footnote{http://www.cvlibs.net/datasets/kitti/}. What is significant by publicly available benchmark data sets is the fact that they, in opposition to strictly private or company owned data sets, indulges to good competition between competing factions and can serve as a fair basis for comparison. Competition and the possibility for comparison will typically motivate and provoke even faster development in an area. And this is exactly what the Pascal VOC data set has done, since the first Pascal VOC challenge started in 2005, for the area of Object Detection among others. It has since been held as a yearly competition, each year presenting new systems which pushed the boundaries for what is possible for computer vision systems. In this report we will be focusing on the Pascal VOC data set which is used for both experiments and evaluation. It should be noted that the Pascal VOC competition was last time held in 2012 and is no longer active. But it still serves as a valid basis for the tasks of this project.  

\subsection{The Pascal VOC data set and Object Detection}
The Pascal VOC data set consists of numerous computer vision challenges, e.g. Image Classification, Object Detection, Instance Segmentation and even more advanced challenges such as Action Classification.   In this project we focus on the Object Detection part of the data set. The Object Detection part of the data set consists of 20 different object classes. For example a person, a bird, an airplane, a chair etc. The goal of the challenge is to recognize both the class and the location of the object in the image. The location of an object is simply represented by the coordinates of a 2D bounding box. The images of the data set depict realistic scenes and have one or several object(s) of a specific class in them.

\subsubsection{Statistics of Pascal VOC}
 

\section{State-of-the-art Object Detection systems: SSD and YOLO}
The following section introduces and describes the Convolutional Neural Network based Object Detection systems, SSD \cite{SSD} and YOLO\cite{YOLO}, which are used and tested in during the project. The systems were, as of 2015-2016, among the state-of-the-art in Object Detection systems and are still today some of the top performing systems.

\subsection{SSD: Single Shot Detection}

\subsection{YOLO: You Only Look Once Detection}


\section{Noise patterns and Object Detection}
The following section introduces different noise patterns and explains the relevance of noise patterns in relation to object detection systems.

\subsection{Why do noisy images matter?}
First of all, let us dwell upon the question of why we are interested in testing how Object Detection system perform on noisy images? Let us consider the case of a vehicle which is driving around on a rainy day. When a human driver is sitting behind the steering wheel the rainy weather unquestionably means it becomes harder for the driver to detect objects he or she sees out of the windows. The rainy weather makes the outside world look different from how it usually looks on a lovely sunny day. Perhaps the traffic lights and the other cars on the road have become blurry and perhaps a raindrop on the car window means a person in a red rain coat, in the distance, looks more like a big round blob of paint than a person. But the human visual perception system is nevertheless still able to deviate from these changes and we can perform the Object Detection task well enough for us to be able to safely navigate around in the traffic, even in rainy weather. Other weather circumstances can likewise change how the world is looking. Consider for instance snowy weather, foggy weather etc. (see figure \ref{â€¢}
So the reason why we are interested in how Object Detection systems perform on noisy images is to figure out how robust the systems are towards small changes in how the input looks. 

\begin{figure}

\end{figure}
  
\subsection{Noisy Patterns}
Another relevant question to ask is what kind of noise patterns will serve as realistic? For us to able to answer this question we analyze and compare some different noise patterns. 

        




% STYLE
%The style files for NIPS and other conference information are
%available on the World Wide Web at
%\begin{center}
%  \url{http://www.nips.cc/}
%\end{center}

% LINKS
%\begin{center}
%  \url{https://google.com/protein+secondary+structure/}
%\end{center}

% PARAGRAPHS
%\paragraph{Paragraphs}
%
%There is also a \verb+\paragraph+ command available, which sets the
%heading in bold, flush left, and inline with the text, with the
%heading followed by 1\,em of space.


% CITATIONS
%
%The \verb+natbib+ package will be loaded for you by default.
%Citations may be author/year or numeric, as long as you maintain
%internal consistency. 
%
%The command \verb+\citet+, which produces citations
%appropriate for use in inline text.  For example,
%\begin{verbatim}
%   \citet{hasselmo} investigated\dots
%\end{verbatim}
%produces
%\begin{quote}
%  Hasselmo, et al.\ (1995) investigated\dots
%\end{quote}

%FOOTNOTES
%Footnotes should be used sparingly. 
%\footnote{Sample of the first footnote.} in the text. 


% FIGURES
%\begin{figure}[h]
%  \centering
%  \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%  \caption{Sample figure caption.}
%\end{figure}


% TABLES
%We strongly suggest the use of the \verb+booktabs+ package,
%which allows for typesetting high-quality, professional tables:
%\begin{center}
%  \url{https://www.ctan.org/pkg/booktabs}
%\end{center}
%This package was used to typeset Table~\ref{sample-table}.
%
%\begin{table}[t]
%  \caption{Sample table title}
%  \label{sample-table}
%  \centering
%  \begin{tabular}{lll}
%    \toprule
%    \multicolumn{2}{c}{Part}                   \\
%    \cmidrule{1-2}
%    Name     & Description     & Size ($\mu$m) \\
%    \midrule
%    Dendrite & Input terminal  & $\sim$100     \\
%    Axon     & Output terminal & $\sim$10      \\
%    Soma     & Cell body       & up to $10^6$  \\
%    \bottomrule
%  \end{tabular}
%\end{table}

%Use unnumbered first-level heading for the references. Any choice of citation 
%style is acceptableas long as you are consistent. 
%\medskip

% References
\bibliographystyle{plain}
\bibliography{references}

\end{document}
